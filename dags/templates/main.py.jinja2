#   mbrs
#   Copyright (c)Cloud Innovation Partners 2020.
#   http://www.cloudinp.com

from plugins.mbrs.utils.dates import cron_presets
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator, BranchPythonOperator
{% if data.email_required is sameas true %}
from plugins.mbrs.operators.common.email_operator import Email_Operator
{% endif %}
import typing
from plugins.mbrs.utils.servicenow import fetch_servicenow_record_count, on_failure,clean_up
from datetime import datetime,timedelta
import pendulum
import airflow.settings
from airflow.models import DagModel

{% if data.storage_type == 'sftp' %}
from plugins.mbrs.operators.servicenow_to_sftp_transfer_operator import ServiceNowToSFTPTransferOperator
{% elif data.storage_type == 's3' %}
from plugins.mbrs.operators.servicenow_to_s3_transfer_operator import ServiceNowToS3TransferOperator
{% elif data.storage_type == 'dropbox' %}
from plugins.mbrs.operators.servicenow_to_dropbox_transfer_operator import ServiceNowToDropboxTransferOperator
{% elif data.storage_type == 'postgres' %}
from plugins.mbrs.operators.servicenow_to_postgres_transfer_operator import ServiceNowToPostgresqlTransferOperator
{% elif data.storage_type == 'mysql' %}
from plugins.mbrs.operators.servicenow_to_mysql_transfer_operator import ServiceNowToMYSQLTransferOperator
{% elif data.storage_type == 'mssql' %}
from plugins.mbrs.operators.servicenow_to_mssql_transfer_operator import ServiceNowToMSSQLTransferOperator

{% endif %}


{%block dag%}
table_name = {{"'"~data.dag_id~"'"}}

dag = DAG(
dag_id=str({{ "'"~data.dag_id|string~"'" }}).replace(' ','_').replace('.','_Dot_').replace('-','_Dash_'),
        schedule_interval={{"'@"~data.frequency~"'"}},
        catchup=True,
        default_args={
            'owner': 'BRS',
            'depends_on_past': False,
            'start_date' : pendulum.parse({{"'"~data.start_date~"'"}}),
            'retries': 1,
            'retry_delay': timedelta(minutes=1)
        },
        params={
            'execution_date' : {% raw %}'{{ execution_date }}'{% endraw %}
        }
)


start = DummyOperator(task_id='start',dag=dag)
end = DummyOperator(task_id='end',dag=dag,trigger_rule='none_failed')
record_count = BranchPythonOperator(task_id='fetch_record_count',python_callable=fetch_servicenow_record_count,provide_context=True,dag=dag,op_kwargs={'table_name': table_name,'execution_date':dag.params.get('execution_date')})
count_less_than_threshold = DummyOperator(task_id='count_within_threshold',dag=dag)
count_exceeds_threshold = DummyOperator(task_id='count_exceeds_threshold',dag=dag)
count_equals_zero = DummyOperator(task_id='count_is_zero',dag=dag)

{% if data.email_required is sameas true%}
notify_submission_failure = Email_Operator(task_id='notify_submission_failure',dag=dag,trigger_rule='all_failed',provide_context=True,python_callable=typing.Callable)
{% endif %}

{% if data.storage_type == 'sftp' %}
submission = ServiceNowToSFTPTransferOperator(task_id='send_data_to_submission',dag=dag,snow_id ='servicenow_default',config = 'config',storage_conn_id = 'sftp_default',table = table_name,execution_date=dag.params.get('execution_date'))
{% elif data.storage_type == 's3' %}
submission = ServiceNowToS3TransferOperator(task_id='send_data_to_submission',dag=dag,snow_id ='servicenow_default',config = 'config',storage_conn_id = 's3_default',table = table_name,execution_date=dag.params.get('execution_date'))
{% elif data.storage_type == 'dropbox' %}
submission = ServiceNowToDropboxTransferOperator(task_id='send_data_to_submission',dag=dag,snow_id ='servicenow_default',config = 'config',storage_conn_id = 'dropbox_default',table = table_name,execution_date=dag.params.get('execution_date'))
{% elif data.storage_type == 'postgres' %}
submission = ServiceNowToPostgresqlTransferOperator(task_id='send_data_to_submission',dag=dag,snow_id ='servicenow_default',config = 'config',storage_conn_id = 'postgres_default',table = table_name,execution_date=dag.params.get('execution_date'))
{% elif data.storage_type == 'mysql' %}
submission = ServiceNowToMYSQLTransferOperator(task_id='send_data_to_submission',dag=dag,snow_id ='servicenow_default',config = 'config',storage_conn_id = 'mysql_default',table = table_name,execution_date=dag.params.get('execution_date'))
{% elif data.storage_type == 'mssql' %}
submission = ServiceNowToMSSQLTransferOperator(task_id='send_data_to_submission',dag=dag,snow_id ='servicenow_default',config = 'config',storage_conn_id = 'mssql_default',table = table_name,execution_date=dag.params.get('execution_date'))

{% endif %}


{% endblock %}

start>>record_count>>[count_less_than_threshold,count_exceeds_threshold,count_equals_zero]
count_less_than_threshold >> submission >> end

{% if data.email_required is sameas true%}
submission.set_downstream(notify_submission_failure)
notify_submission_failure.set_downstream(end)
{%endif%}

end <<[count_exceeds_threshold,count_equals_zero,submission]
{% if data.email_required is sameas true%}
end<<notify_submission_failure
{%endif%}


{% block unpause%}
session = airflow.settings.Session()
try:
    qry = session.query(DagModel).filter(DagModel.dag_id == dag.dag_id)
    d = qry.first()
    d.is_paused = False
    session.commit()
except:
    session.rollback()
finally:
    session.close()
{% endblock %}



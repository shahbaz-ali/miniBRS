#!/usr/bin/python3

#            _       _       ____  ____  ____
#  _ __ ___ (_)_ __ (_)     | __ )|  _ \/ ___|
# | '_ ` _ \| | '_ \| |_____|  _ \| |_) \___ \
# | | | | | | | | | | |_____| |_) |  _ < ___) |
# |_| |_| |_|_|_| |_|_|     |____/|_| \_\____/
#
#  ___           _        _ _       _   _
# |_ _|_ __  ___| |_ __ _| | | __ _| |_(_) ___  _ __
#  | || '_ \/ __| __/ _` | | |/ _` | __| |/ _ \| '_ \
#  | || | | \__ \ || (_| | | | (_| | |_| | (_) | | | |
# |___|_| |_|___/\__\__,_|_|_|\__,_|\__|_|\___/|_| |_|
#

#   mbrs
#   Copyright (c)Cloud Innovation Partners 2020.
#   http://cloudinp.com


import os
import sys
import subprocess
import configparser
import time
import getpass
import shutil
import json

AIRFLOW_HOME = None
AIRFLOW_CONFIG = None
AIRFLOW_DB = None
AIRFLOW_CONFIG_TEST = None

TEMPLATE = """

#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements. See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership. The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# “License”); you may not use this file except in compliance
# with the License. You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied. See the License for the
# specific language governing permissions and limitations
# under the License.
[Unit]
Description={{ DESCRIPTION }}
After=network.target {{ AFTER }}
Wants={{ WANTS }}
[Service]
EnvironmentFile=/etc/environment
User={{ USER }}
Group={{ GROUP }}
Type=simple
ExecStart= {{ EXE }}
Restart={{ RESTART }}
RestartSec=5s
PrivateTmp=true
[Install]
WantedBy=multi-user.target

"""

MESSAGE_TEMPLATE = """

'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
                                                                                       
       MINI_BRS HAS BEEN INSTALLED SUCCESSFULLY                                        
                                                                                       
   Installation Type               : {{INSTALL_TYPE}}                                  
   Project Directory               : {{PROJECT_DIR}}                                   
   Airflow Path                    : {{AIRFLOW_DIR}}                                   
   Meta-Database                   : {{META_DB}}       
   
   airflow.cfg
   ----------------------------------------------------
    [core]
   
    parallelism = 16
    dag_concurrency = 8
    max_active_runs_per_dag = 3    
    secure_mode = True
    [webserver]
    
    worker = 2
    authenticate = True
    auth_backend = airflow.contrib.auth.backends.password_auth
    
    [scheduler]
    
    min_file_process_interval = 60
    dag_dir_list_interval = 100   
                                
                                                                                       
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''

"""

# SqlAlchemy Connection string format :
# <database client> ://<username>:<password>@<host>:<port>/<database>

MYSQL_DEFAULT_PORT = 3306
POSTGRES_DEFAULT_PORT = 5432

AIRFLOW_CONN_MYSQL_DEFAULT = """mysql://{}:{}@{}:{}/{}"""
AIRFLOW_CONN_POSTGRES_DEFAULT = """postgresql://{}:{}@{}:{}/{}"""

INSTALL_TYPE = 'as system-service'
PROJECT_DIR = os.getcwd()
AIRFLOW_DIR = "{}/{}".format(PROJECT_DIR, '.env/bin/')
META_DB = None


def apt_update():
    subprocess.Popen(["apt-get update"],
                     universal_newlines=True, stdout=None, stderr=None, shell=True,
                     executable="/bin/bash").communicate()


def upgrade_setup_tools():
    subprocess.Popen(["pip3 install --upgrade setuptools"],
                     universal_newlines=True, stdout=None, stderr=None, shell=True,
                     executable="/bin/bash").communicate()


def install_pip3():
    subprocess.Popen(["apt-get install -y python3-pip"],
                     universal_newlines=True, stdout=None, stderr=None, shell=True,
                     executable="/bin/bash").communicate()


def install_mysql_client():
    subprocess.Popen(["apt-get install -y libmysqlclient-dev"],
                     universal_newlines=True, stdout=None, stderr=None, shell=True,
                     executable="/bin/bash").communicate()


def check_python_version():
    if sys.version_info.major >= 3 and sys.version_info.minor == 6:
        print("Python version : 3.6 installed OK")
    else:
        print("""

        _______________________________________________________________

                        INSTALLATION TERMINATED                        
        _______________________________________________________________

        You have Python version : 3.5 or below installed ")
        miniBRS uses Airflow version 1.10.9, which requires you to have
        python 3.6 or above, please make sure you have requirements full filled
        for further details checkout the docs section
        """.upper())
        exit(1)


def upgrade_pip():
    process = subprocess.Popen(["pip3 -V"], universal_newlines=True, stdout=subprocess.PIPE, shell=True)
    (output, error) = process.communicate()

    if output.__contains__("pip 9.0.1"):
        print("pip version : 9.0.1  OK")
    else:
        print("pip version : {}".format(output[:9]))
        print("installing pip version 9.0.1")

        p = subprocess.Popen(["python -m pip install --force-reinstall pip==9.0.1"],
                             universal_newlines=True, stdout=subprocess.PIPE, shell=True)
        (o, e) = p.communicate()

        if e is not None:
            print("pip upgrade failed !")
            exit(2)


def install_packages_from_requirement_file(context):
    (out, err) = subprocess.Popen(["sed /pkg-resources==0.0.0/d requirements.txt > output.txt"],
                                  universal_newlines=True, stdout=subprocess.PIPE, shell=True).communicate()

    print('removed pkg-resources==0.0.0 as requirement')
    print('installing requirements....')
    subprocess.check_call([context.get_sys_executable(), "-m", "pip", "install", "-r", "output.txt"])
    subprocess.Popen(["rm output.txt"], universal_newlines=True, stdout=subprocess.PIPE, shell=True).communicate()


def set_airflow_home_as_environment(context):
    if not context.is_virtual:
        global AIRFLOW_HOME

        AIRFLOW_HOME = os.getcwd()

        os.environ["AIRFLOW_HOME"] = AIRFLOW_HOME

        subprocess.Popen(["sed -i '/AIRFLOW_HOME/d' /etc/environment"],
                         universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                         executable="/bin/bash").communicate()

        subprocess.Popen(["sed -i '0,/PATH/ a\\AIRFLOW_HOME=\\\"{}\\\"' /etc/environment".format(AIRFLOW_HOME)],
                         universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                         executable="/bin/bash").communicate()

        subprocess.Popen(["source /etc/environment"],
                         universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                         executable="/bin/bash").communicate()
    else:
        env = os.environ
        env['AIRFLOW_HOME'] = os.getcwd()
        subprocess.Popen(["source .env/bin/activate"], env=env, cwd=os.getcwd(),
                         universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                         executable="/bin/bash").communicate()


def which_database():
    print("""

__________________________________________________________________

        CONFIGURE AIRFLOW META-DATABASE 
__________________________________________________________________

If you are going to use MySQL or Postgres as your database,
before proceeding this step, please ensure you have all the
information in hand related to the database server you are going 
to use. you need to fill the details such as HOST_NAME, USERNAME,
PASSWORD, PORT, DATABASE_NAME in the subsequent steps 

SQLite is not recommended in production environment, The above
message does not apply to SQLite, you can proceed without the
information if you want SQLite as your meta database  

___________________________________________________________________

which database you want to use as airflow meta-database ?

(1) SQLite
(2) MySQL
(3) Postgres

    """.upper())

    global META_DB

    d_id = str(input('Enter your option  (default : 1) '.upper()))

    if d_id == '':
        d_id = 1

    if int(d_id) in (1, 2, 3,):
        from urllib.parse import quote_plus
        connection_string = None

        if int(d_id) == 1:
            META_DB = 'SQLite'
            pass
        elif int(d_id) == 2:
            META_DB = 'MySQL'
            host = str(input("Enter DATABASE HOST NAME (DEFAULT : localhost) : "))
            if host == '':
                host = 'localhost'
            user = str(input("Enter MySQL database username : ".upper()))
            password = getpass.getpass()
            port = str(input("MySQL PORT (DEFAULT : 3306) : "))
            if port == '':
                port = 3306
            database_name = str(input("Enter database name : ".upper()))

            connection_string = AIRFLOW_CONN_MYSQL_DEFAULT.format(
                user, quote_plus(password), host,  port, database_name
            )
        elif int(d_id) == 3:
            META_DB = 'Postgres'
            host = str(input("ENTER DATABASE HOST NAME (DEFAULT : localhost) : "))
            if host == '':
                host = 'localhost'
            user = str(input("Enter Postgres database username : ".upper()))
            password = getpass.getpass()
            port = str(input("Postgres port (default : 5432) : ".upper()))
            if port == '':
                port = 5432
            database_name = str(input("Enter database name : ".upper()))
            connection_string = AIRFLOW_CONN_POSTGRES_DEFAULT.format(
                user, quote_plus(password), host, port, database_name
            )

        return connection_string
    else:
        print('Wrong option ,exiting installation')
        exit(1)


def get_smtp_details():

    print("""

__________________________________________________________________

        CONFIGURE EMAIL NOTIFICATIONS 
__________________________________________________________________

mini-BRS, has email alerting feature on failures, If you want to
use email alerting feature you must have an email account with its
SMTP details.  

If you don't have SMTP details you can skip this step and configure
later via airflow.cfg file.
__________________________________________________________________

    """.upper())

    agree = str(input("Do you want to use email alerting option (default : yes) y/n ? ".upper()))

    if agree == '':
        agree = 'y'

    if agree.lower() == 'y' or agree.lower() == 'yes':
        smtp_host = str(input('SMTP HOST (DEFAULT : smtp.gmail.com) '))
        if smtp_host == '':
            smtp_host = 'smtp.gmail.com'

        smtp_starttls = str(input('ENABLE TLS (DEFAULT : True) True/False ?'))
        if smtp_starttls == '':
            smtp_starttls = 'True'
        smtp_ssl = str(input('ENABLE SSL (DEFAULT : False) True/False ? '))
        if smtp_ssl == '':
            smtp_ssl = 'False'
        smtp_user = str(input('EMAIL ADDRESS :'))
        if smtp_user == '':
            print('smtp_user cannot be left empty !')
        smtp_password = getpass.getpass()
        smtp_port = str(input('PORT (DEFAULT : 587) '))
        if smtp_port == '':
            smtp_port = '587'

        return tuple((smtp_host, smtp_starttls, smtp_ssl, smtp_user, smtp_password, smtp_port,))

    else:
        return None


def modify_airflow_config(context):
    global AIRFLOW_CONFIG
    global AIRFLOW_CONFIG_TEST
    global AIRFLOW_DB

    AIRFLOW_CONFIG = "{}/{}".format(os.getcwd(), 'airflow.cfg')
    AIRFLOW_CONFIG_TEST = "{}/{}".format(os.getcwd(), 'unittests.cfg')
    AIRFLOW_DB = "{}/{}".format(os.getcwd(), 'airflow.db')

    config = configparser.ConfigParser()
    with open(AIRFLOW_CONFIG, 'r') as f:
        config.read_file(f)

        # save database details in config

        connection_string = which_database()

        if connection_string is None:
            pass
        else:
            config.set(
                section='core',
                option='sql_alchemy_conn',
                value=connection_string.replace("%","%%")
            )
            config.set(
                section='core',
                option='executor',
                value='LocalExecutor'
            )

        # save smtp details in config

        smtp_details = get_smtp_details()
        if smtp_details is None:
            print('skipping email configuration !')
        else:
            (smtp_host, smtp_starttls, smtp_ssl, smtp_user, smtp_password, smtp_port,) = smtp_details

            config.set(
                section='smtp',
                option='smtp_host',
                value=smtp_host
            )

            config.set(
                section='smtp',
                option='smtp_starttls',
                value=smtp_starttls
            )

            config.set(
                section='smtp',
                option='smtp_ssl',
                value=smtp_ssl
            )

            config.set(
                section='smtp',
                option='smtp_user',
                value=smtp_user
            )

            config.set(
                section='smtp',
                option='smtp_password',
                value=smtp_password
            )

            config.set(
                section='smtp',
                option='smtp_port',
                value=smtp_port
            )

        # login modifications

        print('setting default values to airflow.cfg...')

        config.set(
            section="core",
            option="load_examples",
            value='False'
        )

        config.set(
            section="webserver",
            option="authenticate",
            value="True"
        )

        config.set(
            section="webserver",
            option="auth_backend",
            value="airflow.contrib.auth.backends.password_auth"
        )

        config.set(
            section="api",
            option="auth_backend",
            value="airflow.contrib.auth.backends.password_auth"
        )

        config.set(
            section="webserver",
            option="workers",
            value='2'
        )

        config.set(
            section="scheduler",
            option="min_file_process_interval",
            value='60'
        )

        config.set(
            section="scheduler",
            option="dag_dir_list_interval",
            value='100'
        )

        config.set(
            section="core",
            option="parallelism",
            value='16'
        )

        config.set(
            section="core",
            option="max_active_runs_per_dag",
            value='3'
        )

        config.set(
            section="core",
            option="dag_concurrency",
            value='8'
        )

        config.set(
            section="core",
            option="secure_mode",
            value="True"
        )

        (username, password,) = create_login_credentails()

        if username == -1 and password == -1:

            print('Error Occurred !')
            exit(1)

        else:
            config.set(
                section='core',
                option='username',
                value=username
            )

            config.set(
                section='core',
                option='password',
                value=password
            )

        f.close()
    os.remove(AIRFLOW_CONFIG)

    with open(AIRFLOW_CONFIG, 'w') as f:
        config.write(f)
        f.close()

    chown(AIRFLOW_CONFIG, os.environ['SUDO_UID'], os.environ['SUDO_GID'])


def create_login_credentails():
    print("""

____________________________________________________________________

    CREATE AIRFLOW ADMIN LOGIN CREDENTIALS
____________________________________________________________________

    """)

    try:
        username = str(input("ENTER USERNAME (DEFAULT : admin) "))
        password = getpass.getpass()
        if username == '':
            username = 'admin'
        return username, password
    except Exception as error:
        print('ERROR', error)
        return -1, -1,


def chown(path, uid, gid):
    os.chown(path=path, uid=int(uid), gid=int(gid))


def chown_r(path, uid):
    env = os.environ
    env['AIRFLOW_HOME'] = os.getcwd()
    subprocess.Popen(["chown -R {}:{} {}".format(uid, uid, path)], cwd=os.getcwd(), env=env,
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()


def airflow_initdb(context):
    print('Initializing airflow meta-database')
    env = os.environ
    env['AIRFLOW_HOME'] = os.getcwd()
    subprocess.Popen(["{} initdb".format(context.get_airflow_executable())], cwd=os.getcwd(), env=env,
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()


def airflow_resetdb(context):
    print('Resetting airflow meta-database')
    subprocess.Popen(["{} resetdb -y".format(context.get_airflow_executable())],
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()


def create_system_service_entries():
    from jinja2 import Template
    import grp
    import pwd

    with open('/etc/systemd/system/airflow-webserver.service', 'w') as f:
        template = Template(TEMPLATE)

        output = template.render(

            DESCRIPTION='airflow web server service',
            AFTER='postgresql.service mysql.service',
            WANTS='postgresql.service mysql.service',
            USER=pwd.getpwuid(int(os.environ['SUDO_UID'])).pw_name,
            GROUP=grp.getgrgid(int(os.environ['SUDO_GID'])).gr_name,
            EXE=str(shutil.which('airflow')) + ' webserver',
            RESTART='on-failure'
        )

        f.write(output)

    with open('/etc/systemd/system/airflow-scheduler.service', 'w') as f:
        template = Template(TEMPLATE)

        output = template.render(

            DESCRIPTION='airflow scheduler service',
            AFTER='postgresql.service mysql.service',
            WANTS='postgresql.service mysql.service',
            USER=pwd.getpwuid(int(os.environ['SUDO_UID'])).pw_name,
            GROUP=grp.getgrgid(int(os.environ['SUDO_GID'])).gr_name,
            EXE=str(shutil.which('airflow')) + ' scheduler',
            RESTART='always'
        )

        f.write(output)


def reload_enable__airflow_webserver_service():
    print("Enabling Airflow Web Server....")

    subprocess.Popen(["systemctl daemon-reload ; systemctl enable airflow-webserver.service"],
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()


def reload_enable_airflow_scheduler_service():
    print("Enabling Airflow Scheduler....")

    subprocess.Popen(["systemctl daemon-reload ; systemctl enable airflow-scheduler.service"],
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()


def start_airflow():
    print("starting mini-BRS")
    subprocess.Popen(["service airflow-webserver start"],
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()
    subprocess.Popen(["service airflow-scheduler start"],
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()


def install_venv_lib():
    print("Installing python3-venv...")
    subprocess.Popen(["apt-get install -y python3-venv"],
                     universal_newlines=True, stdout=None, stderr=None, shell=True,
                     executable='/bin/bash').communicate()


def create_python_virtual_environment():
    print("Initializing python virtual environment ...")
    install_venv_lib()
    subprocess.Popen(["python3 -m venv .env"],cwd=os.getcwd(),
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()

    chown_r('{}/.env'.format(os.getcwd()), os.environ['SUDO_USER'])

    subprocess.Popen(
        ["sed -i \"\\$aexport AIRFLOW_HOME={0}\" {0}/.env/bin/activate ; source {0}/.env/bin/activate".format(os.getcwd())],
        universal_newlines=True, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd(),
        executable='/bin/bash').communicate()


def create_airflow_connection_default_servicenow():
    env = os.environ
    env['AIRFLOW_HOME'] = os.getcwd()
    subprocess.Popen(["{} connections -a --conn_id servicenow_default --conn_type None "
                      "--conn_host https://dev1234.service-now.com --conn_login admin --conn_password None"
                     .format(context.get_airflow_executable())], cwd=os.getcwd(), env=env,
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()


def create_airflow_connection_s3_default():
    env = os.environ
    env['AIRFLOW_HOME'] = os.getcwd()
    subprocess.Popen(["{} connections -a --conn_id s3_default --conn_type None --conn_login access_key_id "
                      "--conn_password None --conn_extra '{}'"
                     .format(context.get_airflow_executable(),
                             json.dumps({"region-name": "ap-south-1", "bucket-name": "mini-brs"}))],
                     cwd=os.getcwd(), env=env,
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()


def create_airflow_connection_dropbox_default():
    env = os.environ
    env['AIRFLOW_HOME'] = os.getcwd()
    subprocess.Popen(["{} connections -a --conn_id dropbox_default --conn_type None --conn_login None "
                      "--conn_password None"
                     .format(context.get_airflow_executable())], cwd=os.getcwd(), env=env,
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()


def create_airflow_connection_googledrive_default():
    env = os.environ
    env['AIRFLOW_HOME'] = os.getcwd()
    subprocess.Popen(["{} connections -a --conn_id google_drive_default --conn_type None --conn_login None --conn_extra '{}' --conn_password None"
                     .format(context.get_airflow_executable(),json.dumps({"access_token": "<YOUR-ACCESS-TOKEN_HERE>","scope": "https://www.googleapis.com/auth/drive","token_type": "Bearer","expires_in": 3599,"refresh_token": "<YOUR-REFRESH-TOKEN_HERE>"}))], cwd=os.getcwd(), env=env,
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()


def create_configuration_variables():

    # 'config' variable

    env = os.environ
    env['AIRFLOW_HOME'] = os.getcwd()
    subprocess.Popen(["{} variables -s config '{}'".format(context.get_airflow_executable(), json.dumps(
        {
            "tables": [],
            "start_date": "1day",
            "frequency": "hourly",
            "threshold": 1000,
            "export_format": "xml",
            "storage_type": "sftp",
            "email": ""
        }))], cwd=os.getcwd(), env=env,
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()

    # 'r_config' variable

    subprocess.Popen(["{} variables -s r_config {}".format(context.get_airflow_executable(), '{}')],
                     cwd=os.getcwd(), env=env,
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()

    # 'dag_creation_dates' variable

    subprocess.Popen(["{} variables -s dag_creation_dates {}".format(context.get_airflow_executable(), '{}')],
                     cwd=os.getcwd(), env=env,
                     universal_newlines=True, stdout=subprocess.PIPE, shell=True,
                     executable='/bin/bash').communicate()


def check_integrity():
    files = [
        'dags',
        'dags/templates/__init__.py',
        'dags/templates/main.py.jinja2',
        'dags/templates/recovery_template.py.jinja2',
        'dags/generated',
        'dags/__init__.py',
        'dags/dag_cleanup.py',
        'dags/dag_generator.py',
        'plugins',
        'plugins/mbrs',
        'plugins/mbrs/blueprints',
        'plugins/mbrs/blueprints/__init__.py',
        'plugins/mbrs/blueprints/recovery_blueprint.py',
        'plugins/mbrs/hooks',
        'plugins/mbrs/hooks/__init__.py',
        'plugins/mbrs/hooks/amazon_s3_hook.py',
        'plugins/mbrs/hooks/servicenow_hook.py',
        'plugins/mbrs/menu_links',
        'plugins/mbrs/menu_links/__init__.py',
        'plugins/mbrs/modals',
        'plugins/mbrs/modals/__init__.py',
        'plugins/mbrs/modals/recovery_modals.py',
        'plugins/mbrs/operators',
        'plugins/mbrs/operators/common',
        'plugins/mbrs/operators/common/__init__.py',
        'plugins/mbrs/operators/common/email_operator.py',
        'plugins/mbrs/operators/common/servicenow_to_generic_transfer_operator.py',
        'plugins/mbrs/operators/__init__.py',
        'plugins/mbrs/operators/servicenow_to_dropbox_transfer_operator.py',
        'plugins/mbrs/operators/servicenow_to_s3_transfer_operator.py',
        'plugins/mbrs/operators/servicenow_to_sftp_transfer_operator.py',
        'plugins/mbrs/operators/servicenow_to_mysql_transfer_operator.py',
        'plugins/mbrs/operators/servicenow_to_mssql_transfer_operator.py',
        'plugins/mbrs/operators/servicenow_to_postgres_transfer_operator.py',
        'plugins/mbrs/operators/servicenow_to_googledrive_transfer_operator.py',
        'plugins/mbrs/templates',
        'plugins/mbrs/templates/recovery_dashboard.html',
        'plugins/mbrs/utils',
        'plugins/mbrs/utils/__init__.py',
        'plugins/mbrs/utils/dates.py',
        'plugins/mbrs/utils/exceptions.py',
        'plugins/mbrs/utils/generator.py',
        'plugins/mbrs/utils/servicenow.py',
        'plugins/mbrs/utils/servicenow_client.py',
        'plugins/mbrs/utils/state.py',
        'plugins/mbrs/views',
        'plugins/mbrs/views/__init__.py',
        'plugins/mbrs/views/recovery_dashboard_view.py',
        'plugins/mbrs/__init__.py',
        'plugins/__init__.py',
        'requirements.txt'
    ]

    missing_files = []
    for item in files:
        if os.path.exists("{}/{}".format(os.getcwd(), item)):
            pass
        else:
            missing_files.append(str(item))
    if len(missing_files) != 0:
        print("mini BRS integrity check FAILED !")
        print("missing files :", str(missing_files))
        exit(1)
    else:
        print("mini BRS integrity check PASSED !")


if __name__ == '__main__':

    check_integrity()

    class Context:

        def __init__(self, is_virtual: bool = False):
            self.is_virtual = is_virtual

        def get_sys_executable(self):

            if self.is_virtual:
                return '.env/bin/python3'
            else:
                return '/usr/bin/python3'

        def get_airflow_executable(self):

            if self.is_virtual:
                return '.env/bin/airflow'
            else:
                return 'airflow'

        def set_environment(self, is_virtual: bool = False):
            self.is_virtual = is_virtual

        def get_environment(self):
            return self.is_virtual


    print("Initializing setup...")

    print("__________________________________________________")
    print("                                                  ")
    print("UPDATING SYSTEM LIBRARIES")
    print("__________________________________________________")

    apt_update()

    check_python_version()

    print("__________________________________________________")
    print("                                                  ")
    print("INSTALLING SYSTEM LIBRARIES")
    print("__________________________________________________")

    install_pip3()
    install_mysql_client()
    upgrade_setup_tools()

    print("_____________________________________________________________________________________")
    print("                                                                                     ")
    print("CREATE VIRTUAL ENVIRONMENT")
    print("_____________________________________________________________________________________")

    print('')
    print('If you are a developer, than you should go for virtual environment installation'.upper())
    print('by choosing virtual installation, mini-brs wont be installed as a service on this'.upper())
    print('machine. You would need to start and stop the service yourself, after activating '.upper())
    print('the virtual environment !'.upper())
    print('')

    v = str(input("Do you want python virtual environment to be created ? (default : No) y/n: ".upper()))

    context = Context()

    if v.lower() == 'y' or v.lower() == 'yes':
        INSTALL_TYPE = 'as python-virtual-environment'
        context.set_environment(is_virtual=True)
        create_python_virtual_environment()

    print("""
  ____ ___ ____                 _       _ ____  ____  ____  
 / ___|_ _|  _ \      _ __ ___ (_)_ __ (_) __ )|  _ \/ ___| 
| |    | || |_) |____| '_ ` _ \| | '_ \| |  _ \| |_) \___ \ 
| |___ | ||  __/_____| | | | | | | | | | | |_) |  _ < ___) |
 \____|___|_|        |_| |_| |_|_|_| |_|_|____/|_| \_\____/ 

    """)


    def spinning_cursor():
        while True:
            for cursor in '|/-\\':
                yield cursor


    spinner = spinning_cursor()

    if os.path.exists("{}/{}".format(os.getcwd(), 'airflow.cfg')):
        print("'airflow.cfg' already exists, changing name to airflow.cfg.old")
        try:
            shutil.move(
                "{}/{}".format(os.getcwd(), 'airflow.cfg'),
                "{}/{}".format(os.getcwd(), 'airflow.cfg.old')
            )
        except Exception as e:
            print("ERROR : Occurred while backing up the file, proceeding to delete")
            print(e)
            os.remove("{}/{}".format(os.getcwd(), 'airflow.cfg'))

    if os.path.exists("{}/{}".format(os.getcwd(), 'airflow.db')):
        print("'airflow.db' already exists, changing name to airflow.db.old")
        try:
            shutil.move(
                "{}/{}".format(os.getcwd(), 'airflow.db'),
                "{}/{}".format(os.getcwd(), 'airflow.db.old')
            )
        except Exception as e:
            print("ERROR : Occurred while backing up the file, proceeding to delete")
            print(e)
            os.remove("{}/{}".format(os.getcwd(), 'airflow.db'))

    if os.path.exists("{}/{}".format(os.getcwd(), 'unittests.cfg')):
        print("'unittests.cfg' already exists, changing name to unittests.cfg.old")
        try:
            shutil.move(
                "{}/{}".format(os.getcwd(), 'unittests.cfg'),
                "{}/{}".format(os.getcwd(), 'unittests.cfg.old')
            )
        except Exception as e:
            print("ERROR : Occurred while backing up the file, proceeding to delete")
            print(e)
            os.remove("{}/{}".format(os.getcwd(), 'unittests.cfg'))

    print("Initializing CIP-miniBRS : gathering requirements...", end='')
    for _ in range(50):
        sys.stdout.write(next(spinner))
        sys.stdout.flush()
        time.sleep(0.1)
        sys.stdout.write('\b')

    # upgrade_pip()

    install_packages_from_requirement_file(context)

    set_airflow_home_as_environment(context)
    shutil.move(src="{}/{}".format(os.getcwd(), 'dags'), dst="{}/{}".format(os.getcwd(), 'bak_dags'))

    try:
        airflow_initdb(context)
        modify_airflow_config(context)

        # Change owner
        chown(AIRFLOW_CONFIG, os.environ['SUDO_UID'], os.environ['SUDO_GID'])
        chown(AIRFLOW_CONFIG_TEST, os.environ['SUDO_UID'], os.environ['SUDO_GID'])
        chown(AIRFLOW_DB, os.environ['SUDO_UID'], os.environ['SUDO_GID'])
        chown_r('{}/logs'.format(os.getcwd()), os.environ['SUDO_USER'])

        os.mkdir("{}/{}".format(os.getcwd(), 'dags'))
        airflow_resetdb(context)

        create_airflow_connection_googledrive_default()
        create_airflow_connection_default_servicenow()
        create_airflow_connection_dropbox_default()
        create_airflow_connection_s3_default()
        create_configuration_variables()
    finally:
        if os.path.exists("{}/{}".format(os.getcwd(), 'dags')):
            shutil.rmtree("{}/{}".format(os.getcwd(), 'dags'))
        if os.path.exists("{}/{}".format(os.getcwd(), 'bak_dags')):
            shutil.move(dst="{}/{}".format(os.getcwd(), 'dags'), src="{}/{}".format(os.getcwd(), 'bak_dags'))

    if not context.is_virtual:
        print("creating service entries for airflow...")
        create_system_service_entries()
        reload_enable__airflow_webserver_service()
        reload_enable_airflow_scheduler_service()
        print("starting airflow as a service...")
        time.sleep(5)
        start_airflow()

    from jinja2 import Template

    if not context.is_virtual:
        AIRFLOW_DIR = shutil.which('airflow')

    template = Template(MESSAGE_TEMPLATE)
    print(template.render(
        INSTALL_TYPE=INSTALL_TYPE,
        PROJECT_DIR=PROJECT_DIR,
        AIRFLOW_DIR=AIRFLOW_DIR,
        META_DB=META_DB
    ))

